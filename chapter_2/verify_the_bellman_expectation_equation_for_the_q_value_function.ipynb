{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "running-serbia",
   "metadata": {},
   "source": [
    "# Verify the Bellman Expectation Equation for the Q-Value Function\n",
    "\n",
    "In the previous lesson, we discussed the Bellman Expectation Equation for the value function. <br><br><br>\n",
    "\n",
    "<center>$\\Large V_{\\pi}(s(t)) = \\mathbf{E}[r(t) + \\gamma V_{\\pi}(s(t+1)) | s(t)]$</center>\n",
    "\n",
    "Actually, there's also a Bellman Expectation Equation for the **Q-value function** (aka action-value function), and it is as follows. <br><br><br>\n",
    "\n",
    "<center>$\\Large Q_{\\pi}(s(t), a(t)) = \\mathbf{E}[r(t) + \\gamma Q_{\\pi}(s(t+1), a(t+1)) | s(t), a(t)]$</center>\n",
    "\n",
    "- Here the expectation $\\mathbf{E}$ is computed over all possible $s(t+1)$ and $a(t+1)$, while following the policy $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-festival",
   "metadata": {},
   "source": [
    "# Task 1: Derive the Bellman Expectation Equation for the Q-value function.\n",
    "\n",
    "- This task is not a coding task. You need to do this with pen and paper.\n",
    "- Drawing diagrams (like I did in the lesson) may help.\n",
    "\n",
    "## How to draw the diagram\n",
    "\n",
    "1. Start with a solid circle representing $Q_{\\pi}(s(t), a(t))$. This solid circle represents taking action $a(t)$ in state $s(t)$.\n",
    "2. The environment will transition to new states in time $t+1$ as a result of the action $a(t)$. Draw hollow circles representing those states. Call one of them $s^{'}$.  $s^{'}$ is a possible $s(t+1)$, and so are the other hollow circles. \n",
    "3. Connect the solid circle to the hollow circles. These connecting lines represent the state transition. Mark the transitions with the rewards $r_{a}^{s(t)s^{'}}$ and the transition probabilities $P_{a}^{s(t)s^{'}}$. The reward $r_{a}^{s(t)s^{'}}$ is a possible $r(t)$, and so are the rewards marked on the other connecting lines.\n",
    "4. Then from new states e.g. $s^{'}$, draw lines to new solid circles. These solid circles represent taking an action in the new states. Call one of the new solid circles $a^{'}$. $a^{'}$ is a possible $a(t+1)$, and so are the other new solid circles.\n",
    "5. Mark the connecting lines between the hollow circles and the new solid circles with the probability of taking actions $\\pi(a^{'} | s^{'})$.\n",
    "6. Finally, try to express $Q_{\\pi}(s(t), a(t))$ (Q-value at solid circle in Step 1) in terms of $Q_{\\pi}(s(t+1), a(t+1))$ (Q-value at solid circles in Step 4).\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-longer",
   "metadata": {},
   "source": [
    "# Task 2: Verify the Bellman Expectation Equation for the Q-value function in `CartPole-v0`.\n",
    "\n",
    "To verify the Bellman Expectation Equation for the Q-Value function, we will first collect the Q-Values for many states using the following strategy. \n",
    "\n",
    "1. We will set up a wrapped environment `pole_right_init_cartpole_env` with the initial state `[0., 0.01, 0.15, 0.]` i.e. the pole tilted to the right at initialization. (code is supplied)\n",
    "2. We will use the epsilon pole direction policy with epsilon 0.9. (code is supplied)\n",
    "3. Using that policy, we will go through 100000 episodes in the wrapped environment and store the action value function for all states we see using the `QValue` helper class. For the `QValue` helper class, we will use $\\gamma=0.8$. (code is supplied)\n",
    "\n",
    "You don't have to write any code for the above. The code is supplied below, just run it. It will store the Q-values  in `q_value_epsilon_pole_direction_policy`, which is an instance of the `QValue` helper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    \"\"\"Wrapper class to change initial state  in CartPole-v0\n",
    "    \"\"\"\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "\n",
    "    \n",
    "# create the wrapped env where the pole it tilted to the right in the initial state    \n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))\n",
    "\n",
    "\n",
    "def get_action_random(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "    \n",
    "def get_action_epsilon_pole_direction_policy(observation):\n",
    "    \"\"\"Sampling function for the epsilon pole direction policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.9:\n",
    "        return get_action_random(observation)\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "class QValue:\n",
    "    def __init__(self, gamma, visit_number=None, q_value_average=None):\n",
    "        self.gamma = gamma\n",
    "        if visit_number is None:\n",
    "            self.visit_number = {}\n",
    "        else:\n",
    "            self.visit_number = visit_number\n",
    "        if q_value_average is None:\n",
    "            self.q_value_average = {}\n",
    "        else:\n",
    "            self.q_value_average = q_value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        backward_reward_sum = 0\n",
    "        for step in reversed(episode_history):\n",
    "            backward_reward_sum = (self.gamma * backward_reward_sum) + step[\"reward\"]\n",
    "            key = (tuple(step[\"observation\"]), step[\"action\"])\n",
    "            try:\n",
    "                visit_number = self.visit_number[key]\n",
    "            except KeyError:\n",
    "                visit_number = 0\n",
    "            if visit_number == 0:\n",
    "                self.q_value_average[key] = backward_reward_sum\n",
    "            else:\n",
    "                self.q_value_average[key] = ((visit_number * self.q_value_average[key]) + backward_reward_sum) / (visit_number + 1)\n",
    "            self.visit_number[key] = visit_number + 1\n",
    "            \n",
    "\n",
    "# compute q-values of states by going through 100000 episodes\n",
    "num_episodes = 100000\n",
    "gamma = 0.8\n",
    "\n",
    "q_value_epsilon_pole_direction_policy = QValue(gamma=gamma)\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = pole_right_init_cartpole_env.reset()\n",
    "    while True:\n",
    "        action = get_action_epsilon_pole_direction_policy(observation)\n",
    "        next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward, \"action\": action})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    q_value_epsilon_pole_direction_policy.update(episode_history)\n",
    "pole_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-baptist",
   "metadata": {},
   "source": [
    "##  $s(0)=[0., 0.01, 0.15, 0.]$ and let's assume $a(0)=0$. In a variable `q_s0_a0`, store the Q-value of this state-action pair by reading the data in `q_value_epsilon_pole_direction_policy`. Print `q_s0_a0`.\n",
    "\n",
    "Your code goes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a variable q_s0_a0, store the q-value of the initial state and action 0 and print it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-scope",
   "metadata": {},
   "source": [
    "## In a variable `s1`, store the next state $s(1)$, if $s(0)=[0., 0.01, 0.15, 0.]$ and  $a(0)=0$.\n",
    "\n",
    "Your code goes in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the variable s_1, store the next state if you take an action 0 in the initial state [0.,0.01,0.15,0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-branch",
   "metadata": {},
   "source": [
    "# The Bellman Expectation equation for the Q-value function for time $t=0$ is given below again for your reference <br><br><br>\n",
    "\n",
    "<center>$\\Large Q_{\\pi}(s(0), a(0)) = \\mathbf{E}[r(0) + \\gamma Q_{\\pi}(s(1), a(1)) | s(0), a(0)]$</center>\n",
    "\n",
    "<br><br>\n",
    "##  I have written an expression below that computes the right hand side of the equation, assuming $s(0)=[0., 0.01, 0.15, 0.]$ and $a(0)=0$. But the expression is not complete. Your job is to complete it.\n",
    "\n",
    "1. The first (leftmost) `1` stands for $E(r(0)| s(0), a(0))$, since the reward in `CartPole-v0` is always `1`.\n",
    "2. The second big expression (partially filled) stands for $E(\\gamma Q_{\\pi}(s(1), a(1) | s(0), a(0))$. \n",
    "3. The `0.8` in the second expression is the gamma.\n",
    "4. Remember that $\\pi$ is the epsilon pole direction policy.\n",
    "\n",
    "Fill in the blanks with numerical values to complete the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the expression that computes the right hand side of the equation. Fill in the blanks with numerical values only.\n",
    "1 + 0.8 * (____ * q_value_epsilon_pole_direction_policy.q_value_average[tuple(s1), 0] + ____ * q_value_epsilon_pole_direction_policy.q_value_average[tuple(s1), 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-yugoslavia",
   "metadata": {},
   "source": [
    "If you did this right, you should get a value very close to `q_s0_a0`, since the right hand side must be equal to `q_s0_a0`, according to the Bellman Expectation Equation.\n",
    "\n",
    "The Bellman Expectation Equation is a foundational equation in RL, and it's awesome that we can reproduce it using `CartPole-v0` and Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

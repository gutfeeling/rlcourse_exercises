{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ethical-steal",
   "metadata": {},
   "source": [
    "# Implementing sampling functions for any environment with discrete actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-overall",
   "metadata": {},
   "source": [
    "## Background: environments which have more than 2 actions\n",
    "\n",
    "Let's say an environment has 3 actions: `0`, `1` and `2`. An example of such an environment is `MountainCar-v0`, which you saw in Chapter 1.\n",
    "\n",
    "In this case, the original `get_action_greedy_policy()` (from the video) won't work anymore. This is because it hardcodes the action choices.\n",
    "\n",
    "```\n",
    "def get_action_greedy_policy(observation, q_value_average):\n",
    "    \"\"\"Sampling function for greedy policy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        q_values = np.array([q_value_average[(tuple(observation), action)] for action in (0, 1)])    # hardcodes action choices\n",
    "    except KeyError:\n",
    "        return get_action_random(observation)    # hardcodes action choices\n",
    "    return np.argmax(q_values)\n",
    "```\n",
    "\n",
    "The `get_action_random()` function also hardcodes the action choices.\n",
    "\n",
    "```\n",
    "def get_action_random(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        return 0    # hardcoding action choices\n",
    "    return 1    # hardcoding action choices\n",
    "```\n",
    "\n",
    "To avoid this hardcoding, we can use `env.action_space.n`, which returns the number of actions allowed in the environment. Run the cells below to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aging-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# env.action_space.n returns 2 for CartPole-v0 because there are two possible actions\n",
    "import gym \n",
    "\n",
    "cartpole_env = gym.make(\"CartPole-v0\")\n",
    "print(cartpole_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# env.action_space.n returns 3 for MountainCar-v0 because there are three possible actions\n",
    "mountaincar_env = gym.make(\"MountainCar-v0\")\n",
    "print(mountaincar_env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-annex",
   "metadata": {},
   "source": [
    "The more general sampling function that works for both `CartPole-v0` and `MountainCar-v0` would do the following.\n",
    "\n",
    "1. Accept `env` as an argument.\n",
    "2. Use `env.action_space.n` and `env.action_space.sample()` to avoid hardcoding the action choices. \n",
    "\n",
    "I have rewritten `get_action_greedy_policy()` below to implement this change. I have called it `get_action_greedy_policy_general()`.\n",
    "\n",
    "Study it carefully and then run the cell to load the function into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chronic-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_action_greedy_policy_general(env, observation, q_value_average):\n",
    "    \"\"\"Sampling function for greedy policy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        q_values = np.array([q_value_average[(tuple(observation), action)] for action in range(env.action_space.n)])    # avoids hardcoding action choices\n",
    "    except KeyError:\n",
    "        return env.action_space.sample()    # avoids hardcoding action choices\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-distributor",
   "metadata": {},
   "source": [
    "Notice that I am still using `np.argmax()`, which means ties won't be broken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-ethiopia",
   "metadata": {},
   "source": [
    "## When there are more actions, we can have more complicated ties!\n",
    "\n",
    "For example, look at the fictitous Q-value dictionary (for a random policy) for `MountainCar-v0` below.  \n",
    "\n",
    "Run the cell to load the dictionary into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compound-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_average_with_ties_complicated = {((0.1, 0.01), 0): -5,    # action 0 doesn't correspond to the max Q-value\n",
    "                                         ((0.1, 0.01), 1): -3,    # action 1 and 2 are tied for the max Q-value\n",
    "                                         ((0.1, 0.01), 2): -3,    \n",
    "                                         ((0.2, 0.02), 0): -7,    # the state np.array([0.2, 0.02]) does not have a tie\n",
    "                                         ((0.2, 0.02), 1): -10,\n",
    "                                         ((0.2, 0.02), 2): -15\n",
    "                                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-madagascar",
   "metadata": {},
   "source": [
    "For the state `np.array([0.1, 0.01])`, actions `1` and `2` are tied for the max Q value. But action `0` has a lower Q-value. In this case, we should break the tie by randomly choosing between `1` and `2`.  We should not choose `0` at all.\n",
    "\n",
    "Of course, the more general sampling function that I wrote `get_action_greedy_policy_general()` still uses `np.argmax()`. So it will return the first index `1` all the time.\n",
    "\n",
    "Verify by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raising-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(get_action_greedy_policy_general(mountaincar_env, np.array([0.1, 0.01]), q_value_average_with_ties_complicated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-giant",
   "metadata": {},
   "source": [
    "## Implement a greedy sampling function with tie breaking that works for any environment with discrete actions (no matter how many)\n",
    "\n",
    "- In the last exercise, you wrote `get_action_greedy_policy_random_tie_break()` taking inspiration from `get_action_greedy_policy()` (from the video).\n",
    "- This would work in the `CartPole-v0` environment.\n",
    "- Now, write a function `get_action_greedy_policy_general_random_tie_break()` that would work for `CartPole-v0`, `MountainCar-v0` or any environment with discrete actions (no matter how many).\n",
    "- You can take inspiration from `get_action_greedy_policy_general()` that I already wrote and modify it to break ties properly.\n",
    "\n",
    "Ready? Your code below below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_greedy_policy_general_random_tie_break(env, observation, q_value_average):\n",
    "    # Implement the greedy policy with random tie breaking for any env with discrete actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-table",
   "metadata": {},
   "source": [
    "## Check if your implementation works as expected\n",
    "\n",
    "Run the cell below. It will run `get_action_greedy_policy_general_random_tie_break()` on the tied state `np.array([0.1, 0.1])` in `MountainCar-v0` 10 times.\n",
    "\n",
    "If you implementation is correct, it choose `1` some of the times, and `2` at other times. It should never choose `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(get_action_greedy_policy_general_random_tie_break(mountaincar_env, \n",
    "                                                            np.array([0.1, 0.01]), \n",
    "                                                            q_value_average_with_ties_complicated\n",
    "                                                            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-carolina",
   "metadata": {},
   "source": [
    "## Did it work? If yes, then congrats! Now we can do policy improvement not just in `CartPole-v0`, but in any environment with discrete actions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

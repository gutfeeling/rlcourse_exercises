{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate value samples for all states visited in an episode for the pole direction policy\n",
    "\n",
    "In the previous lesson, I showed you how to calculate value function samples for all states visited in an episode, while following the random policy.\n",
    "\n",
    "In this exercise, your job is to do the same thing, but for a different policy: the **pole direction policy**.\n",
    "\n",
    "I have included the code for the sampling function of the pole direction policy below, so run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the epsilon pole direction policy, with epsilon = 0.8\n",
    "\n",
    "def get_action_pole_direction_policy(observation):\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use a wrapped `CartPole-v0` environment with a new initial state `[2.4, 0., 0., 0.]`, meaning that the cart is not at the center but is placed towards the right hand side.\n",
    "\n",
    "I have also included the wrapper code below. So run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the wrapped environment called `cart_right_init_cartpole_env`, such that the initital state is the numpy array `[2.4, 0., 0., 0.]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, you should define the cart_right_init_cartpole_env. Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following code to check if the initial state is correct and if the cart is indeed not at the center, but placed to the right upon initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = cart_right_init_cartpole_env.reset()\n",
    "print(observation)\n",
    "cart_right_init_cartpole_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not, then double check your definition of `cart_right_init_cartpole_env` above.\n",
    "\n",
    "And always remember to close the env. So run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the episode history for one complete episode in the `cart_right_init_cartpole_env`.\n",
    "\n",
    "Your job is to loop through a complete episode and collect data on the states and the rewards (just like we did in the lesson). \n",
    "\n",
    "The data will be collected in a list called `episode_history`.\n",
    "\n",
    "Store the state and the reward seen in a single step as a dictionary of the form `{\"observation\": observation, \"reward\": reward}` (again, just like we did in the lesson). `episode_history` should finally contain many such dictionaries (exactly as many as the number of states you see in the episode).\n",
    "\n",
    "Ready? Your code goes below.\n",
    "\n",
    "P.S: I have already set the `episode_history` to an empty list. Now your job is to fill it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_history = []\n",
    "\n",
    "# write code below that follows the pole direction policy for one episode in the cart_right_init_cartpole_env\n",
    "# store the data that you see in episode_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the policy is not random, the environment is deterministic, and the initial state is also not random, I can tell you exactly what you would get if you did this correctly.\n",
    "\n",
    "If you did this correctly, you should have 8 data points in `episode_history` after running your code. \n",
    "\n",
    "`episode_history` should look like the following:\n",
    "\n",
    "```\n",
    "[{'observation': array([2.4, 0. , 0. , 0. ]), 'reward': 1.0}, {'observation': array([ 2.4       , -0.19512195,  0.        ,  0.29268293]), 'reward': 1.0}, {'observation': array([ 2.39609756, -0.3902439 ,  0.00585366,  0.58536585]), 'reward': 1.0}, {'observation': array([ 2.38829268, -0.19520443,  0.01756098,  0.29453263]), 'reward': 1.0}, {'observation': array([ 2.38438859e+00, -3.37189133e-04,  2.34516281e-02,  7.43949311e-03]), 'reward': 1.0}, {'observation': array([ 2.38438185,  0.19444072,  0.02360042, -0.27775288]), 'reward': 1.0}, {'observation': array([ 2.38827066,  0.38921817,  0.01804536, -0.56289982]), 'reward': 1.0}, {'observation': array([ 2.39605503,  0.58408232,  0.00678736, -0.8498434 ]), 'reward': 1.0}]\n",
    "```\n",
    "\n",
    "So check if you also got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally (and this is the last step), calculate the value function sample for this episode for all these 8 states.\n",
    "\n",
    "- Store the value function samples in a dictionary called `value_function_samples`. The keys should be the states (as tuples, not `numpy` arrays). The values should be the value sample for that state obtained in this episode.\n",
    "\n",
    "Ready? Your code goes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value function sample for this episode for all 8 states in episode_history\n",
    "# Store them in a dictionary called value_function_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, because of the deterministic nature of this exercise, I can tell you exactly what `value_function_samples`\n",
    "should look like. It should look like this:\n",
    "\n",
    "```\n",
    "{(2.3960550283158977, 0.5840823170044291, 0.006787363891743958, -0.8498434022904464): 1.0, (2.3882706648964485, 0.38921817097245476, 0.018045360309679223, -0.5628998208967633): 2.0, (2.384381850514366, 0.19444071910413757, 0.023600417978436265, -0.2777528834378521): 3.0, (2.384388594297023, -0.0003371891328543819, 0.023451628116151656, 0.007439493114230422): 4.0, (2.388292682926829, -0.19520443149030264, 0.0175609756097561, 0.2945326253197778): 5.0, (2.3960975609756097, -0.3902439024390244, 0.005853658536585366, 0.5853658536585366): 6.0, (2.4, -0.1951219512195122, 0.0, 0.2926829268292683): 7.0, (2.4, 0.0, 0.0, 0.0): 8.0}\n",
    "```\n",
    "\n",
    "So check if you also got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you did everything right, then congratulations! By computing the value samples, the agent has taken the first step to evaluate its policies and to understand the difference between the many states in the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cross-overall",
   "metadata": {},
   "source": [
    "# Calculate average values of states over many episodes\n",
    "\n",
    "In the lessons, we discusses how $v_{\\pi}(s)$ and $Q_{\\pi}(s, a)$ are expected values. Therefore, we need to average over many value function samples or the Q-value function samples in order to get an accurate estimate for them.\n",
    "\n",
    "In the last lesson, we did exactly that for the Q-value function and calculated the following Q-value for these state-action pairs.\n",
    "\n",
    "| State (s) | Action (a) | Policy ($\\pi$) | $Q_{\\pi}(s, a)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | 1 | random | 8.15 |\n",
    "| `[0, 0.01, 0.15, 0]` | 0 | random | 6.33 |\n",
    "\n",
    "We took the help of a helper class called `QValue` to do this calculation. The code for this class is given below for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValue:\n",
    "    def __init__(self, gamma, visit_number=None, q_value_average=None):\n",
    "        self.gamma = gamma\n",
    "        if visit_number is None:\n",
    "            self.visit_number = {}\n",
    "        else:\n",
    "            self.visit_number = visit_number\n",
    "        if q_value_average is None:\n",
    "            self.q_value_average = {}\n",
    "        else:\n",
    "            self.q_value_average = q_value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        backward_reward_sum = 0\n",
    "        for step in reversed(episode_history):\n",
    "            backward_reward_sum = (self.gamma * backward_reward_sum) + step[\"reward\"]\n",
    "            key = (tuple(step[\"observation\"]), step[\"action\"])\n",
    "            try:\n",
    "                visit_number = self.visit_number[key]\n",
    "            except KeyError:\n",
    "                visit_number = 0\n",
    "            if visit_number == 0:\n",
    "                self.q_value_average[key] = backward_reward_sum\n",
    "            else:\n",
    "                self.q_value_average[key] = (visit_number * self.q_value_average[key] + backward_reward_sum) / (visit_number + 1)\n",
    "            self.visit_number[key] = visit_number + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-newcastle",
   "metadata": {},
   "source": [
    "## In this exercise, we want to do the same thing, but for the value function instead of the action-value function. We want to average over the values of states over many episodes.\n",
    "\n",
    "We want to do this in the following scenario.\n",
    "\n",
    "1. We want to use the `pole_right_init_cartpole_env` (i.e. with initial state `[0., 0.01, 0.15, 0.]`)\n",
    "2. We want to follow the random policy.\n",
    "3. We want to compute averages of value sample obtained in 100000 episodes.\n",
    "4. We want to use $\\gamma=0.9$.\n",
    "\n",
    "The following code should be able to do it. Read it carefully.\n",
    "\n",
    "```\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "        \n",
    "        \n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0., 0.01, 0.15, 0.]))\n",
    "\n",
    "value_info = Value(gamma=0.9)    # this class is not defined yet\n",
    "\n",
    "num_episodes = 100000\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = pole_right_init_cartpole_env.reset()\n",
    "    while True:\n",
    "        action = get_action_random_policy(observation)\n",
    "        next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    value_info.update(episode_history)    # value_info is not defined yet\n",
    "pole_right_init_cartpole_env.close()\n",
    "```\n",
    "\n",
    "But the problem is: we don't have a `Value` class that can calculate the averages of the value samples. We have `QValue` class for the Q-Value function, but not an equivalent class for the value function.\n",
    "\n",
    "Your job is to implement such a `Value` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-billion",
   "metadata": {},
   "source": [
    "## Implement the `Value` class\n",
    "\n",
    "I have provided a skeleton below. Your job is to complete it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, gamma, visit_number=None, value_average=None):\n",
    "        self.gamma = gamma\n",
    "        if visit_number is None:\n",
    "            self.visit_number = {}\n",
    "        else:\n",
    "            self.visit_number = visit_number\n",
    "        if value_average is None:\n",
    "            self.value_average = {}\n",
    "        else:\n",
    "            self.value_average = value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        # implement this method so that the value averages of the states in episode_history is updated\n",
    "        # look at the QValue class for hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-scale",
   "metadata": {},
   "source": [
    "## After you have implemented the `Value` class, run the code below.\n",
    "\n",
    "It should print the value of the state `[0., 0.01, 0.15, 0.]`.\n",
    "\n",
    "If you implemented the `Value` class correctly, you should get a value close to 7.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "        \n",
    "        \n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0., 0.01, 0.15, 0.]))\n",
    "\n",
    "value_info = Value(gamma=0.9)\n",
    "\n",
    "num_episodes = 100000\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = pole_right_init_cartpole_env.reset()\n",
    "    while True:\n",
    "        action = get_action_random_policy(observation)\n",
    "        next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    value_info.update(episode_history)\n",
    "pole_right_init_cartpole_env.close()\n",
    "\n",
    "state = (0., 0.01, 0.15, 0.)\n",
    "print(f\"The value of the state {state} is {value_info.value_average[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-termination",
   "metadata": {},
   "source": [
    "## Now check how many states the agent has seen in these 100000 episodes. Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(value_info.value_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-shift",
   "metadata": {},
   "source": [
    "## Do you see what a whopping huge number that is! The order of magnitude is a few hundred thousands!\n",
    "\n",
    "Computing averages for a significant number of value samples (say 1000 samples for each state) of all these states is going to be unbelievably expensive computationally. And this is the *simplest* Reinforcement Learning environment `CartPole-v0`!\n",
    "\n",
    "This is called \"state space explosion\" :D\n",
    "\n",
    "So what would happen for more complicated environments?\n",
    "\n",
    "If that scares you, don't worry. There is a solution for this. And actually, you have already implemented the solution in one of the earlier assignments!\n",
    "\n",
    "Can you guess which one?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-saskatchewan",
   "metadata": {},
   "source": [
    "# Plot number of state-action pairs seen as a function of number of episodes in exploration mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-tension",
   "metadata": {},
   "source": [
    "# Attention: Please install `matplotlib` before doing this exercise.\n",
    "\n",
    "- Installing it is as simple as running `pip install matplotlib` in your virtualenv.\n",
    "- Once you have installed it, start reading from the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-december",
   "metadata": {},
   "source": [
    "## A key takeway of the previous lesson is that we are not exploring enough. We ran the random policy for `10000` episodes but we still didn't see all state-action pairs in the MDP. Therefore, we should be exploring more.\n",
    "\n",
    "In this exercise, that's exactly what we are going to do. We are going to explore for `100000` episodes. That's `10` times longer compared to what we did before. We will discover many more state-action pairs!\n",
    "\n",
    "\n",
    "### Your job is to plot the number of state-action pairs that we see as a function of number of episodes.\n",
    "\n",
    "I have already supplied most of helper functions and classes that you will need below. Run the cell to load them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action_random(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "class QValue:\n",
    "    \"\"\"Helper for computing Q-value of state-action pairs. \n",
    "    It has an update() method that updates averages of Q-value samples with new episode data\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, visit_number=None, q_value_average=None):\n",
    "        self.gamma = gamma\n",
    "        if visit_number is None:\n",
    "            self.visit_number = {}\n",
    "        else:\n",
    "            self.visit_number = visit_number\n",
    "        if q_value_average is None:\n",
    "            self.q_value_average = {}\n",
    "        else:\n",
    "            self.q_value_average = q_value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        backward_reward_sum = 0\n",
    "        for step in reversed(episode_history):\n",
    "            backward_reward_sum = (self.gamma * backward_reward_sum) + step[\"reward\"]\n",
    "            key = (tuple(step[\"observation\"]), step[\"action\"])\n",
    "            try:\n",
    "                visit_number = self.visit_number[key]\n",
    "            except KeyError:\n",
    "                visit_number = 0\n",
    "            if visit_number == 0:\n",
    "                self.q_value_average[key] = backward_reward_sum\n",
    "            else:\n",
    "                self.q_value_average[key] = (visit_number * self.q_value_average[key] + \n",
    "                                             backward_reward_sum\n",
    "                                             ) / (visit_number + 1)\n",
    "            self.visit_number[key] = visit_number + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-austria",
   "metadata": {},
   "source": [
    "I have also set up the wrapped environment where we will be performing this experiment. Run the cell below to load this into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    \"\"\"Wrapper class to change initial state  in CartPole-v0\n",
    "    \"\"\"\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "    \n",
    "\n",
    "# create the wrapped env    \n",
    "wrapped_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-fever",
   "metadata": {},
   "source": [
    "I have also written a loop where the agent will take random actions for `100000` episodes and calculate the Q values for all state-action pairs seen using a helper called `q_value_random_policy`. \n",
    "\n",
    "Here's what you need to do.\n",
    "\n",
    "1. I have setup two empty 1D `numpy` arrays before starting the loop. They are called `x_num_episodes` and `y_num_state_action_pairs`. \n",
    "2. Anytime the agent completes a multiple of `1000` episodes, you should append the current episode number to `x_num_episodes`. You should also, at the same time, append the total number of state-action pairs seen so far to `y_num_state_action_pairs`.\n",
    "\n",
    "Ready? Your code goes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100000\n",
    "gamma = 0.95\n",
    "\n",
    "q_value_greedy_policy = QValue(gamma=gamma)\n",
    "\n",
    "x_num_episodes = np.array([])\n",
    "y_num_state_action_pairs = np.array([])\n",
    "\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = wrapped_env.reset()\n",
    "    while True:\n",
    "        action = get_action_random(observation)\n",
    "        next_observation, reward, done, _ = wrapped_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward, \"action\": action})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    q_value_greedy_policy.update(episode_history)\n",
    "    # Your code goes here. If the episode number is a multiple of 1000, append to x_num_episodes and y_num_state_action_pairs\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-university",
   "metadata": {},
   "source": [
    "# Run the cell below to plot how the number of unique state-action pairs grows with the number of episodes in exploration mode\n",
    "\n",
    "- This should work if you collected the data properly in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_num_episodes, y_num_state_action_pairs)\n",
    "ax.set(xlabel=\"number of episodes\", ylabel=\"number of unique state-action pairs seen\",\n",
    "       title=\"Growth of state-action pairs in exploration mode\"\n",
    "       )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-starter",
   "metadata": {},
   "source": [
    "# You should see an almost linear growth of states. The max number of state-action pairs seen (the data for the `100000`th episode) should be several hundred thousands.\n",
    "\n",
    "If you got the same results, congrats! Now you see how we discover so many new state-action pairs as we explore the environment more. We won't be able to solve `CartPole-v0` unless the agent sees all the various state-action pairs in the MDP and understands how good or bad they are. Thus, exploration plays a crucial role in solving a RL problem, along with policy improvement (exploiting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-radar",
   "metadata": {},
   "source": [
    "# Calculate value samples using the discounted reward sum formula (and not by merely adding up the rewards)\n",
    "\n",
    "In this exercise, we are going to continue right from where we stopped in the last exercise. Here's the details to jog your memory.\n",
    "\n",
    "1. We were using a wrapped `CartPole-v0` environment with the initial state `[2.4, 0., 0., 0.]`.\n",
    "2. We were following the pole direction policy.\n",
    "3. We executed one episode following this policy, and stored the data (states and rewards) in a list called `episode_history`.\n",
    "\n",
    "I have supplied all the code up till this point below. So, simply run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def get_action_pole_direction_policy(observation):\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "    \n",
    "    \n",
    "cart_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([2.4, 0., 0., 0.]))\n",
    "\n",
    "\n",
    "episode_history = []\n",
    "observation = cart_right_init_cartpole_env.reset()\n",
    "while True:\n",
    "    action = get_action_pole_direction_policy(observation)\n",
    "    next_observation, reward, done, _ = cart_right_init_cartpole_env.step(action)\n",
    "    episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "    observation = next_observation\n",
    "    if done:\n",
    "        break\n",
    "cart_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-weight",
   "metadata": {},
   "source": [
    "As we know, `episode_history` has data on 8 states.\n",
    "\n",
    "## Your job is to calculate the value function sample for this episode for all these 8 states. But this time, value is defined as the discounted reward sum with $\\gamma=0.8$\n",
    "\n",
    "- Store the value function samples in a dictionary called `value_function_samples`. The keys should be the states (as tuples, not `numpy` arrays). The values should be the value sample (using the discounted reward sum formula) for that state obtained in this episode.\n",
    "- Use $\\gamma=0.8$.\n",
    "\n",
    "Ready? Your code goes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value function sample for this episode for all 8 states in episode_history, using gamma = 0.8\n",
    "# Store them in a dictionary called value_function_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-telephone",
   "metadata": {},
   "source": [
    "If you did everything right, you should get the following `value_function_samples` dictionary.\n",
    "\n",
    "```\n",
    "{(2.3960550283158977, 0.5840823170044291, 0.006787363891743958, -0.8498434022904464): 1.0, (2.3882706648964485, 0.38921817097245476, 0.018045360309679223, -0.5628998208967633): 1.8, (2.384381850514366, 0.19444071910413757, 0.023600417978436265, -0.2777528834378521): 2.4400000000000004, (2.384388594297023, -0.0003371891328543819, 0.023451628116151656, 0.007439493114230422): 2.9520000000000004, (2.388292682926829, -0.19520443149030264, 0.0175609756097561, 0.2945326253197778): 3.3616000000000006, (2.3960975609756097, -0.3902439024390244, 0.005853658536585366, 0.5853658536585366): 3.6892800000000006, (2.4, -0.1951219512195122, 0.0, 0.2926829268292683): 3.9514240000000007, (2.4, 0.0, 0.0, 0.0): 4.161139200000001}\n",
    "```\n",
    "\n",
    "So check if you got this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-office",
   "metadata": {},
   "source": [
    "## If you did everything right, then congrats! Discounted reward sum is great because it converges faster and avoids too much speculation about the very uncertain rewards far into the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

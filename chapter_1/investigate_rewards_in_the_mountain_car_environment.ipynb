{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate rewards in the `MountainCar-v0` environment\n",
    "\n",
    "In the video lesson, we learned that the rewards at any time step depend on the environment state and the action.\n",
    "\n",
    "In this exercise, you will investigate the reward function of the `MountainCar-v0` environment by taking random actions for 100 steps and printing out observation, action and the corresponding rewards.\n",
    "\n",
    "I have already created the environment for you in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what I want you to do.\n",
    "\n",
    "1. Write a loop where the agent takes random actions for **100** steps.\n",
    "2. Don't forget to initialize the environment before you begin taking actions.\n",
    "3. We need to keep track of the environment state *in which the action is taken*, and not the environment state after the action is taken. Therefore, before taking the action, store the environment state in which the action will be taken in a variable `observation_during_action`.\n",
    "4. Instead of using `env.action_space.sample()` directly inside `env.step()`, store the return value of `env.action_space.sample()` in a variable `action` first and call `env.step(action)` to take the action.\n",
    "5. After taking the action, print out `previous_observation`, `action` and the reward returned by `env.step()` in a single line.\n",
    "\n",
    "You should get a 100 line output like this when you run the loop.\n",
    "\n",
    "```\n",
    "observation: [-0.56491169  0.00131887], action: 2, reward: -1.0\n",
    "observation: [-0.56228377  0.00262792], action: 2, reward: -1.0\n",
    "...\n",
    "...\n",
    "...\n",
    "```\n",
    "\n",
    "You write your code in the cell below. GO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a loop where the agent takes 100 random actions and print out the observation, action and the corresponding reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the reward depends on the observation and action from the output of the loop. From the output, can you guess the reward function (or shall we say, punishment function) of the `MountainCar-v0` environment?\n",
    "\n",
    "## Here's a hint, but you still need to fill in the blank\n",
    "\n",
    "The reward does not depend on the environment state and the action! It is a constant with the value ____ at every time step.\n",
    "\n",
    "## How do you maximize such a reward function?\n",
    "\n",
    "That's a good question. The answer will come after a few more lessons. So hold that thought and just carry on with the lessons. \n",
    "\n",
    "But hey, if you managed to print the desired output correctly, then you have surely become very good at writing `gym` programs. This is an essential skill for writing Reinforcement Learning code, so, hurray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you used `env.render()` in your loop to visualize what's happening, the code below will close the graphical window\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
